import os
# è¨­ç½®ç’°å¢ƒè®Šé‡ä»¥è§£æ±º OpenMP é‡è¤‡åˆå§‹åŒ–å•é¡Œ
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
# é™åˆ¶ PyTorch ä½¿ç”¨çš„ç·šç¨‹æ•¸
os.environ['OMP_NUM_THREADS'] = '1'
import gymnasium as gym
import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import matplotlib.pyplot as plt
from IPython import display

# è¨­ç½® PyTorch ç·šç¨‹æ•¸
torch.set_num_threads(4)

# è¨­ç½®ä¸­æ–‡å­—é«”æ”¯æŒ
plt.rcParams['font.family'] = ['Microsoft YaHei', 'SimHei', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False  # è§£æ±ºè² è™Ÿé¡¯ç¤ºå•é¡Œ


# è¨­ç½®è¶…åƒæ•¸
GAMMA = 0.99       # æŠ˜æ‰£å› å­
LR = 1e-2          # å­¸ç¿’ç‡
BATCH_SIZE = 128    # æ‰¹æ¬¡å¤§å°
MEMORY_SIZE = 10000 # è¨˜æ†¶å›æ”¾å®¹é‡
EPSILON_START = 1.0  # åˆå§‹æ¢ç´¢ç‡
EPSILON_END = 0.01   # æœ€ä½æ¢ç´¢ç‡
EPSILON_DECAY = 0.995 # æ¢ç´¢ç‡è¡°æ¸›
TARGET_UPDATE = 5   # ç›®æ¨™ç¶²çµ¡æ›´æ–°é »ç‡
EPISODES = 500       # è¨“ç·´å›åˆæ•¸
RENDER_EVERY = 50    # æ¯éš”å¤šå°‘å›åˆæ¸²æŸ“ä¸€æ¬¡ç’°å¢ƒ
# æª¢æ¸¬ä¸¦è¨­ç½®è¨­å‚™ (CPU æˆ– GPU)
# æ·»åŠ æ›´è©³ç´°çš„ GPU æª¢æ¸¬ä¿¡æ¯

print("CUDA æ˜¯å¦å¯ç”¨:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("CUDA ç‰ˆæœ¬:", torch.version.cuda)
    print("GPU å‹è™Ÿ:", torch.cuda.get_device_name(0))
    device = torch.device("cuda")
else:
    print("æœªæª¢æ¸¬åˆ° GPUï¼Œä½¿ç”¨ CPU é€²è¡Œè¨“ç·´")
    print("PyTorch ç‰ˆæœ¬:", torch.__version__)
    device = torch.device("cpu")

print(f"ä½¿ç”¨è¨­å‚™: {device}")

# æª¢æ¸¬ä¸¦è¨­ç½®è¨­å‚™ (CPU æˆ– GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è¨­å‚™: {device}")

# ğŸ® 1. ç’°å¢ƒå°è£
env = gym.make("CartPole-v1")
state_dim = env.observation_space.shape[0]  # ç’°å¢ƒçš„ç‹€æ…‹ç¶­åº¦ (4)
action_dim = env.action_space.n             # è¡Œå‹•æ•¸é‡ (2)

# ğŸ§  2. å®šç¾© DQN ç¥ç¶“ç¶²çµ¡
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)  # ç¬¬ä¸€å±¤éš±è—å±¤
        self.fc2 = nn.Linear(128, 128)        # ç¬¬äºŒå±¤éš±è—å±¤
        self.fc3 = nn.Linear(128, action_dim) # è¼¸å‡ºå±¤ï¼šæ¯å€‹å‹•ä½œçš„ Q å€¼

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)  # è¼¸å‡º Q å€¼

# ğŸ¯ 3. DQN Agent
class DQNAgent:
    def __init__(self, state_dim, action_dim):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.epsilon = EPSILON_START  # åˆå§‹æ¢ç´¢ç‡
        self.device = device  # ä½¿ç”¨æª¢æ¸¬åˆ°çš„è¨­å‚™

        # å…©å€‹ç¥ç¶“ç¶²çµ¡ï¼ˆä¸»ç¶²çµ¡å’Œç›®æ¨™ç¶²çµ¡ï¼‰- ç¢ºä¿ç§»å‹•åˆ° GPU
        self.policy_net = DQN(state_dim, action_dim).float().to(self.device)
        self.target_net = DQN(state_dim, action_dim).float().to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())  # åŒæ­¥åƒæ•¸

        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LR)  # Adam å„ªåŒ–å™¨
        self.memory = deque(maxlen=MEMORY_SIZE)  # è¨˜æ†¶å›æ”¾ç·©å­˜

    # ğŸ“Œ å­˜å„²ç¶“é©—
    def store_experience(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    # ğŸ¤– é¸æ“‡å‹•ä½œï¼ˆÎµ-greedy ç­–ç•¥ï¼‰
    def select_action(self, state):
        if random.random() < self.epsilon:  # æ¢ç´¢
            return random.randint(0, self.action_dim - 1)
        else:  # åˆ©ç”¨ Q å€¼
            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)
            with torch.no_grad():
                return torch.argmax(self.policy_net(state)).item()

    # ğŸš€ è¨“ç·´ DQN
    def train(self):
        if len(self.memory) < BATCH_SIZE:
            return  # è¨˜æ†¶åº«ä¸å¤ æ™‚ä¸è¨“ç·´

        # éš¨æ©Ÿå–æ¨£
        batch = random.sample(self.memory, BATCH_SIZE)
        states, actions, rewards, next_states, dones = zip(*batch)

        # è½‰æ›ç‚ºå¼µé‡ä¸¦ç§»å‹•åˆ° GPU
        states = torch.tensor(states, dtype=torch.float32).to(self.device)
        actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1).to(self.device)
        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)
        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)
        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)

        # è¨ˆç®—ç•¶å‰ Q å€¼
        q_values = self.policy_net(states).gather(1, actions).squeeze(1)

        # è¨ˆç®—ç›®æ¨™ Q å€¼ï¼ˆä½¿ç”¨ target_netï¼‰åœ¨è¨“ç·´å‰å·²é‡‡å–äº†è¡Œå‹•å¾—åˆ°äº†next_states
        with torch.no_grad():
            max_next_q_values = self.target_net(next_states).max(1)[0]
            # dones == 1æ™‚gameoverï¼Œä¸ç²å¾—çå‹µï¼Œç¶²çµ¡ä¸æœƒæ›´æ–°
            target_q_values = rewards + GAMMA * max_next_q_values * (1 - dones)

        # æå¤±å‡½æ•¸ & åå‘å‚³æ’­
        loss = nn.MSELoss()(q_values, target_q_values)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()  # è¿”å›æå¤±å€¼

    # ğŸ”„ æ›´æ–°ç›®æ¨™ç¶²çµ¡
    def update_target_net(self):
        self.target_net.load_state_dict(self.policy_net.state_dict())

    # ğŸ“‰ æ›´æ–°æ¢ç´¢ç‡
    def update_epsilon(self):
        self.epsilon = max(EPSILON_END, self.epsilon * EPSILON_DECAY)

# ğŸ”¥ 4. è¨“ç·´ä¸»å‡½æ•¸
def train_dqn():
    agent = DQNAgent(state_dim, action_dim)

    # ç”¨æ–¼è¨˜éŒ„è¨“ç·´æ•¸æ“šçš„åˆ—è¡¨
    rewards_history = []
    avg_rewards_history = []
    epsilon_history = []
    losses = []
    
    # ä¸é å…ˆå‰µå»ºæ¸²æŸ“ç’°å¢ƒï¼Œè€Œæ˜¯åœ¨éœ€è¦æ™‚å‰µå»º
    render_env = None

    plt.figure(figsize=(12, 8))

    # æ¯ä¸€å±€
    for episode in range(EPISODES):
        state = env.reset()[0]  # å–å¾—åˆå§‹ç‹€æ…‹
        total_reward = 0
        episode_losses = []
        
        # æ¯ä¸€å¹€
        while True:
            action = agent.select_action(state)  # é¸æ“‡å‹•ä½œ
            next_state, reward, done, _, _ = env.step(action)  # èˆ‡ç’°å¢ƒäº¤äº’
            agent.store_experience(state, action, reward, next_state, done)  # å­˜å„²ç¶“é©—

            state = next_state
            total_reward += reward

            # è¨“ç·´ä¸¦è¨˜éŒ„æå¤±
            if len(agent.memory) >= BATCH_SIZE:
                loss = agent.train()  # è¨“ç·´ DQNï¼Œä¸¦è¿”å›æå¤±å€¼
                if loss is not None:
                    episode_losses.append(loss)

            if done:
                break

        # æ¯ 5 å›åˆæ›´æ–°ç›®æ¨™ç¶²çµ¡
        if episode % TARGET_UPDATE == 0:
            agent.update_target_net()

        agent.update_epsilon()  # æ›´æ–°æ¢ç´¢ç‡

        # è¨˜éŒ„è¨“ç·´æ•¸æ“š
        rewards_history.append(total_reward)
        avg_reward = np.mean(rewards_history[-100:])  # æœ€è¿‘100å›åˆçš„å¹³å‡çå‹µ
        avg_rewards_history.append(avg_reward)
        epsilon_history.append(agent.epsilon)
        
        if episode_losses:
            losses.append(np.mean(episode_losses))
        
        print(f"Episode {episode}: Total Reward: {total_reward}, Avg Reward: {avg_reward:.2f}, Epsilon: {agent.epsilon:.2f}")
        
        # æ¯éš”ä¸€å®šå›åˆæ•¸ç¹ªè£½è¨“ç·´æ›²ç·š
        if episode % 10 == 0:
            plt.clf()
            
            # ç¹ªè£½çå‹µæ›²ç·š
            plt.subplot(2, 2, 1)
            plt.plot(rewards_history)
            plt.plot(avg_rewards_history)
            plt.title('çå‹µéš¨å›åˆè®ŠåŒ–')
            plt.xlabel('å›åˆ')
            plt.ylabel('çå‹µ')
            plt.legend(['æ¯å›åˆçå‹µ', 'å¹³å‡çå‹µ'])
            
            # ç¹ªè£½æ¢ç´¢ç‡æ›²ç·š
            plt.subplot(2, 2, 2)
            plt.plot(epsilon_history)
            plt.title('æ¢ç´¢ç‡éš¨å›åˆè®ŠåŒ–')
            plt.xlabel('å›åˆ')
            plt.ylabel('æ¢ç´¢ç‡ (Îµ)')
            
            # ç¹ªè£½æå¤±æ›²ç·š
            if losses:
                plt.subplot(2, 2, 3)
                plt.plot(losses)
                plt.title('æå¤±éš¨å›åˆè®ŠåŒ–')
                plt.xlabel('å›åˆ')
                plt.ylabel('æå¤±')
            
            plt.tight_layout()
            plt.savefig(f'f:/projects/DouZero/training_progress.png')
            display.clear_output(wait=True)
            display.display(plt.gcf())
        
        # æ¯éš” RENDER_EVERY å›åˆæ¸²æŸ“ä¸€æ¬¡ç’°å¢ƒ
        if episode % RENDER_EVERY == 0 and episode > 0:
            # åœ¨éœ€è¦æ™‚å‰µå»ºæˆ–é‡æ–°å‰µå»ºæ¸²æŸ“ç’°å¢ƒ
            render_env = gym.make("CartPole-v1", render_mode="human")
            visualize_agent(agent, render_env)
            # ä½¿ç”¨å¾Œç«‹å³é—œé–‰
            try:
                render_env.close()
            except:
                pass

    plt.close()
    
    # ä¿å­˜è¨“ç·´å¥½çš„æ¨¡å‹
    torch.save(agent.policy_net.state_dict(), 'f:/projects/DouZero/dqn_cartpole_model.pth')
    
    # ç¹ªè£½æœ€çµ‚çš„è¨“ç·´æ›²ç·š
    plot_training_results(rewards_history, avg_rewards_history, epsilon_history, losses)


def visualize_agent(agent, render_env, max_steps=500):
    """æ¸²æŸ“ç’°å¢ƒä»¥å¯è¦–åŒ–æ™ºèƒ½é«”çš„è¡¨ç¾"""
    try:
        # é‡æ–°å‰µå»ºæ¸²æŸ“ç’°å¢ƒä»¥é¿å… pygame éŒ¯èª¤
        render_env.close()
        render_env = gym.make("CartPole-v1", render_mode="human")
        
        state = render_env.reset()[0]
        total_reward = 0
        
        for t in range(max_steps):
            # ä½¿ç”¨ç•¶å‰ç­–ç•¥é¸æ“‡å‹•ä½œï¼ˆç„¡æ¢ç´¢ï¼‰- ç§»å‹•åˆ° GPU
            with torch.no_grad():
                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(agent.device)
                action = torch.argmax(agent.policy_net(state_tensor)).item()
            
            next_state, reward, done, _, _ = render_env.step(action)
            total_reward += reward
            state = next_state
            
            if done:
                break
        
        print(f"å¯è¦–åŒ–å›åˆçå‹µ: {total_reward}")
    except Exception as e:
        print(f"æ¸²æŸ“éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
    finally:
        # ç¢ºä¿ç’°å¢ƒè¢«æ­£ç¢ºé—œé–‰
        try:
            render_env.close()
        except:
            pass

# ç¹ªè£½æœ€çµ‚çš„è¨“ç·´çµæœ
def plot_training_results(rewards, avg_rewards, epsilons, losses):
    """ç¹ªè£½ä¸¦ä¿å­˜å®Œæ•´çš„è¨“ç·´çµæœåœ–è¡¨"""
    plt.figure(figsize=(15, 10))
    
    plt.subplot(2, 2, 1)
    plt.plot(rewards)
    plt.plot(avg_rewards)
    plt.title('è¨“ç·´çå‹µ')
    plt.xlabel('å›åˆ')
    plt.ylabel('çå‹µ')
    plt.legend(['æ¯å›åˆçå‹µ', 'å¹³å‡çå‹µ'])
    
    plt.subplot(2, 2, 2)
    plt.plot(epsilons)
    plt.title('æ¢ç´¢ç‡è®ŠåŒ–')
    plt.xlabel('å›åˆ')
    plt.ylabel('æ¢ç´¢ç‡ (Îµ)')
    
    if losses:
        plt.subplot(2, 2, 3)
        plt.plot(losses)
        plt.title('è¨“ç·´æå¤±')
        plt.xlabel('å›åˆ')
        plt.ylabel('æå¤±')
    
    plt.tight_layout()
    plt.savefig('f:/projects/DouZero/final_training_results.png')
    plt.show()


# ğŸš€ åŸ·è¡Œè¨“ç·´
if __name__ == "__main__":
    train_dqn()
